#!/bin/bash
# Ollama Startup Script for STM32H753ZI Development Environment
# Ensures Ollama service runs persistently with GPU acceleration

set -e

echo "ğŸš€ Starting Ollama service for STM32H753ZI development..."
echo "ğŸ“Š GPU: RTX 4080 SUPER (14.7 GiB available)"
echo "ğŸ§  Models: mxbai-embed-large (669MB), nomic-embed-text (274MB)"

# Check if Ollama is already running
if pgrep -f "ollama serve" > /dev/null; then
    echo "âœ… Ollama is already running (PID: $(pgrep -f 'ollama serve'))"
    echo "ğŸ”— Available at: http://127.0.0.1:11434"
    exit 0
fi

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama not found. Please install Ollama first."
    exit 1
fi

# Start Ollama service
echo "ğŸ”§ Starting Ollama service..."
echo "   Host: 127.0.0.1:11434"
echo "   GPU: CUDA with RTX 4080 SUPER"
echo "   Context: STM32H753ZI semantic search"

# Start Ollama in background with proper logging
nohup ollama serve > /tmp/ollama.log 2>&1 &
OLLAMA_PID=$!

# Wait for service to start
echo "â³ Waiting for Ollama to initialize..."
sleep 3

# Verify service is running
if pgrep -f "ollama serve" > /dev/null; then
    echo "âœ… Ollama started successfully (PID: $OLLAMA_PID)"
    echo "ğŸ“‹ Log file: /tmp/ollama.log"
    
    # Test connection
    echo "ğŸ§ª Testing connection..."
    sleep 2
    if curl -s http://127.0.0.1:11434/api/tags > /dev/null; then
        echo "âœ… Ollama API is accessible"
        
        # Show available models
        echo "ğŸ“š Available embedding models:"
        ollama list | grep -E "(mxbai-embed|nomic-embed)" || echo "   (No embedding models found - please pull mxbai-embed-large)"
        
        echo ""
        echo "ğŸ¯ Ollama is ready for semantic search!"
        echo "   Usage: python scripts/stm32_semantic_search.py concept 'GPIO configuration' --scope STM32H7"
    else
        echo "âš ï¸  Ollama started but API not responding yet. Give it a moment..."
    fi
else
    echo "âŒ Failed to start Ollama service"
    exit 1
fi

echo "ğŸ“ To stop Ollama: pkill -f 'ollama serve'"
echo "ğŸ“Š To monitor: tail -f /tmp/ollama.log"
